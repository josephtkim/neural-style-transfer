{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS2BSpOIQCyi"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1397,
      "metadata": {
        "id": "CgG2xyHqQEBp"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1398,
      "metadata": {
        "id": "fluKXCO72R1T"
      },
      "outputs": [],
      "source": [
        "DIR = \"drive/MyDrive/Colab Notebooks/Projects/Neural Style Transfer/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZpWeQwwOnA4"
      },
      "source": [
        "### Preprocess the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1399,
      "metadata": {
        "id": "MFop0gG2_U1z"
      },
      "outputs": [],
      "source": [
        "content_image_path = keras.utils.get_file(\"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\")\n",
        "\n",
        "style_image_path = keras.utils.get_file(\"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1400,
      "metadata": {
        "id": "ITYJXYBCYi8R"
      },
      "outputs": [],
      "source": [
        "# Images\n",
        "content_image = keras.preprocessing.image.load_img(content_image_path)\n",
        "style_image = keras.preprocessing.image.load_img(style_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1401,
      "metadata": {
        "id": "0ayFSVs_XTyZ"
      },
      "outputs": [],
      "source": [
        "width, height = content_image.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1402,
      "metadata": {
        "id": "_FQ9yKMgXk2u"
      },
      "outputs": [],
      "source": [
        "# Target dimensions for generated image.\n",
        "rows = 400\n",
        "cols = int(width * (rows / height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1403,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O8e6zVZXacA",
        "outputId": "fa767c49-c102-4d9c-b3e8-d62ce04280fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400 711\n"
          ]
        }
      ],
      "source": [
        "print(rows, cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1404,
      "metadata": {
        "id": "SfitEzJlNlec"
      },
      "outputs": [],
      "source": [
        "# Convert image to tensor\n",
        "def preprocess_image(image_path):\n",
        "  img = keras.preprocessing.image.load_img(\n",
        "      image_path, target_size=(rows, cols)\n",
        "  )\n",
        "  img = keras.preprocessing.image.img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  img = vgg19.preprocess_input(img)\n",
        "  return tf.convert_to_tensor(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1405,
      "metadata": {
        "id": "jKT32_FhQvtj"
      },
      "outputs": [],
      "source": [
        "# Convert tensor to image\n",
        "def unprocess_image(X):\n",
        "  X = X.reshape((rows, cols, 3))\n",
        "  # Remove zero-center by mean pixel\n",
        "  # VGG network requires images to be zero mean.\n",
        "  X[:, :, 0] += 103.939\n",
        "  X[:, :, 1] += 116.779\n",
        "  X[:, :, 2] += 123.68\n",
        "  # 'BGR' -> 'RGB'\n",
        "  X = X[:, :, ::-1]\n",
        "  X = np.clip(X, 0, 255).astype(\"uint8\")\n",
        "  return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VThJiUiDQ_G_"
      },
      "source": [
        "### Create the VGG19 model with pre-trained ImageNet weights, without the top layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1406,
      "metadata": {
        "id": "fXQZIIMpLkGU"
      },
      "outputs": [],
      "source": [
        "vgg = vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1407,
      "metadata": {
        "id": "Ne21cdZQcgyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6472cb60-0cb4-45cb-ad81-f93ca60f166f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_142\n",
            "block1_conv1\n",
            "block1_conv2\n",
            "block1_pool\n",
            "block2_conv1\n",
            "block2_conv2\n",
            "block2_pool\n",
            "block3_conv1\n",
            "block3_conv2\n",
            "block3_conv3\n",
            "block3_conv4\n",
            "block3_pool\n",
            "block4_conv1\n",
            "block4_conv2\n",
            "block4_conv3\n",
            "block4_conv4\n",
            "block4_pool\n",
            "block5_conv1\n",
            "block5_conv2\n",
            "block5_conv3\n",
            "block5_conv4\n",
            "block5_pool\n"
          ]
        }
      ],
      "source": [
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1408,
      "metadata": {
        "id": "8-Px9od_RoWc"
      },
      "outputs": [],
      "source": [
        "# Content layers matched\n",
        "content_layers = [\n",
        "    'block4_conv1',\n",
        "    #'block5_conv1'\n",
        "    ]\n",
        "\n",
        "# Style layers matched\n",
        "style_layers = [\n",
        "    'block1_conv1',\n",
        "    'block2_conv1',\n",
        "    'block3_conv1',\n",
        "    'block4_conv1',\n",
        "    #'block5_conv1'\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1409,
      "metadata": {
        "id": "3Guggsv1ffIB"
      },
      "outputs": [],
      "source": [
        "# Given a model, returns the feature outputs when called with \n",
        "# some image input.\n",
        "class FeatureExtractor(keras.models.Model):\n",
        "  def __init__(self, layer_names):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.vgg = vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "    self.vgg.trainable = False\n",
        "    self.outputs = dict([(name, self.vgg.get_layer(name).output) for name in layer_names])\n",
        "    self.model = tf.keras.Model([self.vgg.input], self.outputs)\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1410,
      "metadata": {
        "id": "ptDblQTT8oZY"
      },
      "outputs": [],
      "source": [
        "content_extractor = FeatureExtractor(layer_names=content_layers)\n",
        "style_extractor = FeatureExtractor(layer_names=style_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1411,
      "metadata": {
        "id": "IzemowWM-Yat"
      },
      "outputs": [],
      "source": [
        "content_image_processed = preprocess_image(content_image_path)\n",
        "style_image_processed = preprocess_image(style_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1412,
      "metadata": {
        "id": "UDx_4M3f80Dr"
      },
      "outputs": [],
      "source": [
        "content_outputs = content_extractor(content_image_processed)\n",
        "style_outputs = style_extractor(style_image_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1413,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkCtajLD9wPc",
        "outputId": "07e545ad-21dc-4a45-b582-1a35b45a420c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block4_conv1 (1, 50, 88, 512)\n"
          ]
        }
      ],
      "source": [
        "for layer_name in content_outputs:\n",
        "  print(layer_name, content_outputs[layer_name].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1414,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGbwe9G7AY6V",
        "outputId": "b15ab51f-7e9f-4a5d-c2d5-b4d42f32fb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block1_conv1 (1, 400, 711, 64)\n",
            "block2_conv1 (1, 200, 355, 128)\n",
            "block3_conv1 (1, 100, 177, 256)\n",
            "block4_conv1 (1, 50, 88, 512)\n"
          ]
        }
      ],
      "source": [
        "for layer_name in style_outputs:\n",
        "  print(layer_name, style_outputs[layer_name].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3jxOZNUQOFW"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1415,
      "metadata": {
        "id": "Qb_gD8AWQ2mD"
      },
      "outputs": [],
      "source": [
        "# Returns the Gram matrix\n",
        "def get_gram_matrix(X):\n",
        "  C = X.get_shape().as_list()[-1]\n",
        "  reshaped = tf.reshape(X, [-1, C])\n",
        "  G = tf.matmul(reshaped, reshaped, transpose_a=True)\n",
        "  return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1416,
      "metadata": {
        "id": "Z5M99uheY8iH"
      },
      "outputs": [],
      "source": [
        "# P = C(content), F = C(generated)\n",
        "def calc_content_loss(P, F):\n",
        "  #return tf.reduce_sum(tf.square(F - P)) / 2.0\n",
        "  return tf.reduce_mean(tf.square(F - P)) / 2.0\n",
        "\n",
        "# A = S(style), G = S(generated) for a layer\n",
        "def calc_style_loss(A, G):\n",
        "  A_gram = get_gram_matrix(A)\n",
        "  G_gram = get_gram_matrix(G)\n",
        "  shape = A.get_shape().as_list()\n",
        "  N_l = shape[-1]\n",
        "  M_l = shape[1] * shape[2]\n",
        "  return tf.reduce_mean(tf.square(G_gram - A_gram)) / (4.0 * (N_l**2) * (M_l**2))\n",
        "\n",
        "# From https://keras.io/examples/generative/neural_style_transfer/\n",
        "# This is used for visual coherence\n",
        "def total_variation_loss(X):\n",
        "  A = tf.square(\n",
        "      X[:, : rows - 1, : cols - 1, :] - X[:, 1:, : cols - 1, :]\n",
        "  )\n",
        "  B = tf.square(\n",
        "      X[:, : rows - 1, : cols - 1, :] - X[:, : rows - 1, 1:, :]\n",
        "  )\n",
        "  return tf.reduce_sum(tf.pow(A + B, 1.25))\n",
        "  #return tf.reduce_mean(tf.pow(A + B, 1.25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1417,
      "metadata": {
        "id": "GYK4p8ApFiNq"
      },
      "outputs": [],
      "source": [
        "# The ratio alpha/beta is 1e-3 or 1e-4 in the paper\n",
        "\n",
        "# Content weight\n",
        "alpha = 1e1\n",
        "# Style weight\n",
        "beta = 1e7\n",
        "# Total variation weight\n",
        "total_variation_weight = 5e-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7QZ91cNG2Gl"
      },
      "source": [
        "Store the target features for the content image and style image. These only need to be calculated once since the network is not going to change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1418,
      "metadata": {
        "id": "R5Tee1J8GkhN"
      },
      "outputs": [],
      "source": [
        "content_target = content_extractor(content_image_processed)\n",
        "style_target = style_extractor(style_image_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1419,
      "metadata": {
        "id": "5MM210muH47x"
      },
      "outputs": [],
      "source": [
        "N_content = len(content_layers)\n",
        "N_style = len(style_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1420,
      "metadata": {
        "id": "A-zxvQlXFE8j"
      },
      "outputs": [],
      "source": [
        "def compute_loss(x):\n",
        "  content_features = content_extractor(x)\n",
        "  style_features = style_extractor(x)\n",
        "\n",
        "  # Content loss\n",
        "  content_loss = 0.0\n",
        "  for layer in content_features:\n",
        "    content_loss += (calc_content_loss(content_target[layer], content_features[layer]) / N_content)\n",
        "\n",
        "  # Style loss\n",
        "  style_loss = 0.0\n",
        "  for layer in style_features:\n",
        "    style_loss += (calc_style_loss(style_target[layer], style_features[layer]) / N_style)\n",
        "  \n",
        "  loss = alpha*content_loss + beta*style_loss\n",
        "\n",
        "  # Total variation loss\n",
        "  loss += total_variation_weight * total_variation_loss(x)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUM-5hotyuzM"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1421,
      "metadata": {
        "id": "oDfQgysVVDp0"
      },
      "outputs": [],
      "source": [
        "lr = 1\n",
        "EPOCHS = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1422,
      "metadata": {
        "id": "1-dsSleGy4CX"
      },
      "outputs": [],
      "source": [
        "# Adam optimizer\n",
        "opt_adam = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1423,
      "metadata": {
        "id": "0g-qWOm2XnSW"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(x)\n",
        "  grads = tape.gradient(loss, x)\n",
        "  return loss, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1424,
      "metadata": {
        "id": "WL86fSPbXT_S"
      },
      "outputs": [],
      "source": [
        "combination_image = tf.Variable(content_image_processed)\n",
        "\n",
        "def train(epochs):\n",
        "  for epoch in range(1, epochs+1):\n",
        "    loss, grads = train_step(combination_image)\n",
        "    opt_adam.apply_gradients([(grads, combination_image)])\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      clear_output()\n",
        "      print(f'Iteration: {epoch}, loss: {loss}')\n",
        "      \n",
        "  img = unprocess_image(combination_image.numpy())\n",
        "  fname = DIR + \"/images/1-1.png\"\n",
        "  keras.preprocessing.image.save_img(fname, img)\n",
        "  display(Image(fname))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(EPOCHS)"
      ],
      "metadata": {
        "id": "_lRDnyjX3WhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}